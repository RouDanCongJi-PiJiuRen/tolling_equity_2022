{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook develops Suits index (Measure of tax (here toll!) progressivity) at city level. Suits index is calculated by measuring the area between lorenz curve and proportional line (45 degree line.) Value is between -1 (regressive-fare burden is on low-income individuals) and 1 (progressive-fare burden is on high-income individuals).\n",
    "Preprocessing step (sql):\n",
    "Need to run sql code \"Suits_index_city.sql\" to have a table with trip data and the corresponding cities' name (TOT_W_NULL_C)\n",
    "Here we develop a mixed effect model (Dependant variable is fair paid during one year($) and independent variables include (fixed effect: cities, hh income, and interaction between cities and hh income, random effect: tracts.) Variables are log-scaled\n",
    "We simulate the hhs'income from the distribution of cities and plug into the model to calculate the fair share for the hh and then use income and fair to plot the lorenz curve.\n",
    "Here we only did analysis on the 9 city with the highest number of trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysqlcipher3 import dbapi2 as sqlite\n",
    "from os import environ\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keynow = environ['HOT_KEY']\n",
    "conn = sqlite.connect('hot_2.1.db')\n",
    "conn.execute('pragma key=\\\"x\\''+keynow+'\\'\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe that includes fips, corresponding city, their share of toll, number of accounts, and number of their trips from that fips\n",
    "dffips = pd.read_sql_query(\"SELECT fips, tract, city_new_final, SUM(toll),count(Distinct(id)),\n",
    "                           count(Distinct(trip_id)) FROM TOT_W_NULL_C GROUP BY fips\", conn)\n",
    "dffips.rename(columns = {'city_new_final':'new_city'},inplace = True)\n",
    "dffips['fareshare'] = dffips['SUM(toll)']/dffips['count(Distinct(id))']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading census block group data calculate mean income \n",
    "dfcensus = pd.read_csv('block_group_census_estimates_wide_original_bins.csv')\n",
    "dfcensus['mean_income_hh'] = (dfcensus['pc_income']*dfcensus['population'])/dfcensus['households']\n",
    "dfcensus.rename(columns = {'fips_code':'fips'},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging the the fips table and census block group table \n",
    "dfcensus['fips'] = dfcensus['fips'].astype(str)\n",
    "dffips['fips'] = dffips['fips'].astype(str)\n",
    "dfmergetemp = pd.merge(dffips,dfcensus,on='fips')\n",
    "dfmergetemp['log_scaled_mean_income_hh'] = np.log(dfmergetemp['mean_income_hh'])\n",
    "dfmergetemp['log_fareshare'] = np.log(dfmergetemp['fareshare'])\n",
    "dfmerge = dfmergetemp[['fips','tract_x','new_city','SUM(toll)','count(Distinct(id))','count(Distinct(trip_id))','fareshare','county_name','mean_income_hh','log_scaled_mean_income_hh','log_fareshare']]\n",
    "dfmerge.rename(columns={'tract_x':'tract','new_city':'city','SUM(toll)':'total_toll','count(Distinct(id))':'total_accounts',\n",
    "                        'count(Distinct(trip_id))':'total_trips','fareshare':'fare_per_account','mean_income_hh':'mean_hh_income',\n",
    "                        'log_scaled_mean_income_hh':'log_scaled_mean_hh_income','log_fareshare':'log_scaled_fare_per_account'},inplace = True)\n",
    "# Removing fips code without city\n",
    "dfmerge_w_na = dfmerge.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Looking at cities with highest number of trips\n",
    "dfmerge_w_na = dfmerge_w_na[dfmerge_w_na['city'].isin(['BOTHELL','KIRKLAND','EVERETT','LYNNWOOD','WOODINVILLE','SEATTLE','BELLEVUE','SNOHOMISH','REDMOND'])]\n",
    "dfmerge_w_na.sort_values(by = ['total_trips'],ascending = False, inplace = True)\n",
    "dfmerge_w_na['tract_name'] = 'tract' + dfmerge_w_na['tract'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plotting\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.set_palette(sns.color_palette(\"hls\", 9))\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "sns.lmplot(x = 'log_scaled_mean_hh_income',y='log_scaled_fare_per_account',hue='city',data= dfmerge_w_na, height = 10)\n",
    "plt.savefig('Figures/incomevfare')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelling\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "model = smf.mixedlm(\" log_scaled_fare_per_account ~ log_scaled_mean_hh_income*city \", dfmerge_w_na, groups= \"tract_name\")\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is inspired by Cory's code on fitting a distribution to a histogram\n",
    "from sympy import Symbol\n",
    "from scipy.optimize import minimize \n",
    "from scipy.stats import burr\n",
    "from scipy import stats\n",
    "\n",
    "bins = np.array([0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 75, 100, 125, 150, 200, np.inf])\n",
    "n_bins = len(bins) - 1\n",
    "bins_l = bins[0:n_bins]\n",
    "bins_r = bins[1:(n_bins + 1)]\n",
    "\n",
    "def burr_fcn(params,counts_in,bins_r_in,bins_left_in):\n",
    "    estimated_percentage = (burr.cdf(bins_r_in, c= params[0], d = params[1], loc=0, scale=params[2]) -  burr.cdf(bins_left_in, c= params[0], d = params[1], loc=0, scale=params[2]))\n",
    "    estimated_percentage = np.where(estimated_percentage <= 1e-99 ,1e-99, estimated_percentage)\n",
    "    log_lik = -np.sum(counts_in*100*np.log(estimated_percentage))\n",
    "    return(log_lik)\n",
    "\n",
    "def opt_fcn(fcn_in,bins_l_in,counts_in, city_in):\n",
    "    bnds = ((0, None), (0, None), (0, None))\n",
    "    # Needed to change initial start point for WOODINVILLE\n",
    "    if(city_in == 'WOODINVILLE'):\n",
    "        return minimize(fcn_in, np.array([2,1,120]),args = (counts_in,bins_r,bins_l),bounds=bnds) # method = 'BFGS\n",
    "    else:\n",
    "        return minimize(fcn_in, np.array([4,0.6,120]),args = (counts_in,bins_r,bins_l),bounds=bnds) # method = 'BFGS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lorenz_curve(fare_share_in,income_share_in,num_share_in,axes_in,ind_i_in,ind_j_in,name_in):\n",
    "    cum_fare_share = np.cumsum(fare_share_in)\n",
    "    cum_num_share = np.cumsum(num_share_in)\n",
    "    cum_income_share = np.cumsum(income_share_in)\n",
    "    l1 = axes_in[ind_i_in,ind_j_in].plot(cum_income_share*100,cum_fare_share*100,color = 'red', label = 'Lorenz curve (Toll burden)')\n",
    "    l2 = axes_in[ind_i_in,ind_j_in].plot(cum_income_share*100,cum_num_share*100,color = 'black',  label = 'Lorenz curve (Income inequality)')\n",
    "    l3 = axes_in[ind_i_in,ind_j_in].plot(np.arange(0,1.1,0.1)*100,np.arange(0,1.1,0.1)*100,color = 'blue',  label = 'Perfect Equality')\n",
    "    axes_in[ind_i_in,ind_j_in].set_title(str(name_in))\n",
    "    if (name_in =='REDMOND'):\n",
    "        legend.extend([l1,l2,l3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_suits_index(fare_share_in,income_share_in):\n",
    "    cum_fare_share = np.cumsum(fare_share_in)\n",
    "    cum_income_share = np.cumsum(income_share_in)\n",
    "    suits_index = 0\n",
    "    for i in range(1,cum_fare_share.shape[0]):\n",
    "        suits_index = suits_index + 0.5*(cum_fare_share[i]+cum_fare_share[i-1])*(cum_income_share[i]-cum_income_share[i-1])\n",
    "    return 2*(1/2-suits_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins0 = np.array([0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 75, 100, 125, 150, 200])\n",
    "bins1 = np.array([10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 75, 100, 125, 150, 200, 1000])\n",
    "bins2 = np.array([10, 5, 5, 5, 5, 5, 5, 5, 5, 10, 15, 25, 25, 25, 50, 800])\n",
    "legend =[]\n",
    "dfcensus = pd.read_csv('block_group_census_estimates_wide_original_bins.csv')\n",
    "n1 = 3\n",
    "n2 = 3\n",
    "fig, axes = plt.subplots(n1,n2,figsize=(12,12),sharex=True,sharey=True)\n",
    "fig1, ax1 = plt.subplots(n1,n2,figsize=(12,12),sharex=True,sharey=True)\n",
    "indi = 0\n",
    "indj = 0\n",
    "for i in ['BELLEVUE','BOTHELL','KIRKLAND','EVERETT','LYNNWOOD','WOODINVILLE','SEATTLE','SNOHOMISH','REDMOND']:\n",
    "    dffips =  dfmerge_w_na[ dfmerge_w_na['city'] == i].fips\n",
    "    dfnum =  dfmerge_w_na[ dfmerge_w_na['city'] == i][['fips','total_accounts']]\n",
    "    dfcensus_city = dfcensus[dfcensus['fips_code'].isin(dffips)]\n",
    "    dfcensus_income = dfcensus_city[['fips_code','inc_000_010k', 'inc_010_015k', 'inc_015_020k',\n",
    "       'inc_020_025k', 'inc_025_030k', 'inc_030_035k', 'inc_035_040k',\n",
    "       'inc_040_045k', 'inc_045_050k', 'inc_050_060k', 'inc_060_075k',\n",
    "       'inc_075_100k', 'inc_100_125k', 'inc_125_150k', 'inc_150_200k',\n",
    "       'inc_200_infk']]\n",
    "    dfnum.set_index('fips', inplace = True)\n",
    "    dfnum.sort_index(inplace = True)\n",
    "    dfcensus_income.set_index('fips_code', inplace = True)\n",
    "    dfcensus_income.sort_index(inplace = True)\n",
    "    dfproduct = dfcensus_income[['inc_000_010k', 'inc_010_015k', 'inc_015_020k',\n",
    "       'inc_020_025k', 'inc_025_030k', 'inc_030_035k', 'inc_035_040k',\n",
    "       'inc_040_045k', 'inc_045_050k', 'inc_050_060k', 'inc_060_075k',\n",
    "       'inc_075_100k', 'inc_100_125k', 'inc_125_150k', 'inc_150_200k',\n",
    "       'inc_200_infk']].mul(dfnum[\"total_accounts\"].values,axis='index')\n",
    "    dfcensusincome = dfproduct.sum()/dfnum['total_accounts'].sum()\n",
    "    counts = dfcensusincome\n",
    "    count = np.sum(counts)\n",
    "    # Finding the optimal parameters for the distribution \n",
    "    result = opt_fcn(burr_fcn,bins_l,counts,i)\n",
    "    if result.success:\n",
    "        newbins = np.array([0, 10, 15, 20, 25, 30, 35, 40, 45, 50, 60, 75, 100, 125, 150, 200,1000])\n",
    "        x1 = np.linspace(0,1000,1000)\n",
    "        y1 = burr.pdf(x1, c = result.x[0] , d=result.x[1] , scale = result.x[2])\n",
    "        ax1[indi,indj].bar(bins0, np.array(dfcensusincome/bins2), width = bins2, ec=\"r\", align=\"edge\", alpha = 0.2)\n",
    "        ax1[indi,indj].plot(x1,y1)\n",
    "        ax1[indi,indj].set_title(str(i))\n",
    "        simnum = 10000000\n",
    "        r = burr.rvs(c = result.x[0] , d=result.x[1] , scale = result.x[2], size=simnum)\n",
    "        # Sorting income\n",
    "        income_sim_sort =  np.sort(r*1000)\n",
    "        # Calcualting the log\n",
    "        income_sim = np.log(income_sim_sort)\n",
    "        # Fixing the paramters needed for applying the fitted mixed model\n",
    "        name = 'T.' + str(i)\n",
    "        city_coeff = 'city[' + name  + ']'\n",
    "        income_coeff = 'log_scaled_mean_hh_income:city[' + name + ']'\n",
    "        if(i == 'BELLEVUE'):\n",
    "            fare = np.exp(mdf.params.get('Intercept') + mdf.params.get('log_scaled_mean_hh_income')* income_sim)\n",
    "        else:\n",
    "            fare = np.exp(mdf.params.get('Intercept') + mdf.params.get(city_coeff) + mdf.params.get(income_coeff)* income_sim +\n",
    "                          mdf.params.get('log_scaled_mean_hh_income')* income_sim)\n",
    "        final = pd.DataFrame({'fare_share':fare ,'income_share':income_sim_sort,'num_share':np.repeat(1,simnum)})\n",
    "        final['fare_share_norm']= final['fare_share']/final['fare_share'].sum()\n",
    "        final['income_share_norm']= final['income_share']/final['income_share'].sum()\n",
    "        final['num_share_norm'] = final['num_share']/final['num_share'].sum()\n",
    "        plot_lorenz_curve(final['fare_share_norm'],final['income_share_norm'],final['num_share_norm'],axes,indi,indj,i)\n",
    "        if(indi < n1-1):\n",
    "            indi = indi + 1\n",
    "        else:\n",
    "            indi = 0\n",
    "            indj = indj + 1\n",
    "        print(\"done\")\n",
    "    else:\n",
    "        raise ValueError(result.message)\n",
    "fig.text(0.06, 0.5, 'Accumulated % of total fare or Accumulated % of total accounts', va='center', rotation='vertical')\n",
    "fig.text(0.5, 0.06, 'Accumulated % of total income', va='center', rotation='horizontal')\n",
    "axes[0,2].legend(legend,['Lorenz curve (Toll burden)','Lorenz curve (Income inequality)','Perfect Equality'],loc=\"upper right\")\n",
    "plotname1 = 'Figures/suits_index'\n",
    "plotname2 = 'Figures/distribution'\n",
    "fig.savefig(plotname1) \n",
    "fig1.savefig(plotname2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
