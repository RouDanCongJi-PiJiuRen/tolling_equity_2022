{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working dir: /opt/dssg-hot/notebooks/shirley\n"
     ]
    }
   ],
   "source": [
    "import altair as alt\n",
    "from functools import reduce\n",
    "import gc\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pysqlcipher3 import dbapi2 as sqlcipher\n",
    "#import seaborn as sns\n",
    "import shapely\n",
    "\n",
    "path = os.getcwd()\n",
    "print('working dir: ' + path)\n",
    "#/opt/dssg-hot/notebooks/shirley"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get and compute trip data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in hot-v3 db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pysqlcipher3.dbapi2.Cursor at 0x7f67f8008f10>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - $HOT_KEY exists in environment already\n",
    "keynow = os.environ['HOT_KEY']\n",
    "db = sqlcipher.connect('/opt/dssg-hot/data/shirleydata/sqldbs/hot-v3.db')\n",
    "db.execute('pragma key=\\\"x\\''+keynow+'\\'\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-9223314567028386566,\n",
       "  1,\n",
       "  'NEWCASTLE',\n",
       "  'WA',\n",
       "  98059,\n",
       "  1,\n",
       "  530330250051,\n",
       "  33,\n",
       "  25005,\n",
       "  1031)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Test if the HOT_KEY password worked\n",
    "db.execute('select * from census limit 1;').fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Read in joined trip data\n",
    "df = pd.read_sql_query(\"select trip_id, toll, entry_time, exit_time, entry_plaza, exit_plaza, is_hov, tag_id, acct, plate, id, plate_state, fips from trips_linked;\", db)\n",
    "#df.head()\n",
    "#list(df.columns)\n",
    "#df.shape\n",
    "#df.dtypes\n",
    "#df.isna().sum() # look at how many nans per column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Change trip_id, tag_id, acct, plate, id to strings\n",
    "# df['trip_id']=df['trip_id'].astype(str)\n",
    "# df['tag_id']=df['tag_id'].astype(str)\n",
    "# df['acct']=df['acct'].astype(str)\n",
    "# df['plate']=df['plate'].astype(str)\n",
    "# df['id']=df['id'].astype(str)\n",
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Join dist traveled, start/end mile posts, travel time, entry speed/volume w/ trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Merge distance traveled + start/end mile posts for each trip\n",
    "entry_exit_plu = pd.read_csv('/opt/dssg-hot/data/milepost_lookups/trips_entry_exit_summary.csv')\n",
    "#entry_exit_plu['Count'] = entry_exit_plu['Count'].str.replace(',','').astype(int) # only need this if commas were in #s\n",
    "entry_exit_plu['Percentage'] = entry_exit_plu['Percentage'].str.replace('%','').astype(float)\n",
    "entry_exit_plu['Dist btwn entry & exit loop'] = abs(entry_exit_plu['Dist btwn entry & exit loop'])\n",
    "entry_exit_plu.rename(columns={'Entry Plaza':'entry_plaza', 'Exit Plaza':'exit_plaza'}, inplace=True)\n",
    "\n",
    "df = pd.merge(\n",
    "    df, entry_exit_plu[['entry_plaza','exit_plaza','Actual entry mile post','Actual exit mile post','Dist btwn entry & exit loop']],\n",
    "    how='left', on=['entry_plaza','exit_plaza']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # - Merge travel time + entry speed/volume for each trip \n",
    "# def roundarb(x, base):\n",
    "#     return base * round(x/base)\n",
    "# tt = pd.read_csv('../../data/shirleydata/TRACFLOW_travel_times/concatenated_405_travel_times_5am_to_8pm_5min.csv')\n",
    "# tt.head()\n",
    "# df['entry_time_5min']=roundarb(df['entry_time'], base=300).astype(int)\n",
    "# pd.to_datetime(df['entry_time_5min'], unit='s')\n",
    "# tt\n",
    "# # for a, b in zip(df['entry_time'],roundarb(df['entry_time'])):\n",
    "# #     print(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute groups to filter on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Commerical vs. private via number of tag_ids per id</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test commercial vs. private classification; following # should be zero: 0\n"
     ]
    }
   ],
   "source": [
    "# --> id's w/ >6 diff tag_id's = commercial\n",
    "\n",
    "# - Note that NA groups in groupby are automatically excluded\n",
    "id_tagid_cts = df.groupby('id')['tag_id'].nunique()\n",
    "comm_ids = id_tagid_cts[id_tagid_cts>6].index.values\n",
    "df['is_commercial_by_num_tags'] = df['id'].isin(comm_ids).astype(int)\n",
    "# - OR: # dfnc = df[~df['id'].isin(comm_ids)] # df, no commercial trips\n",
    "\n",
    "# - Test:\n",
    "# print(id_tagid_cts.sort_values(ascending=False))\n",
    "# print(len(comm_ids))\n",
    "# print(len(id_tagid_cts))\n",
    "# print(id_tagid_cts[id_tagid_cts>6].index.values)\n",
    "comm_ids_test = df[df['is_commercial_by_num_tags']==0].groupby('id')['tag_id'].nunique()\n",
    "# - OR: # comm_ids_test = dfnc.groupby('id')['tag_id'].nunique()\n",
    "print('Test commercial vs. private classification; following # should be zero: ' + str(comm_ids_test[comm_ids_test > 6].count())) # should be zero\n",
    "\n",
    "# - Usage:\n",
    "# Get df w/o commerical by num tags --> dfnew = df[df['is_commercial_by_num_tags']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Commerical vs. private via number of trips per id</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test commerical vs. private classification by num of trips; following # should be zero: 0\n"
     ]
    }
   ],
   "source": [
    "# --> id's w/ >10k trips = commercial\n",
    "\n",
    "uf_id_cts = df['id'].value_counts() # counts how many times each value appears in the column = # of trips per id\n",
    "comm_by_num_trips_ids = uf_id_cts[uf_id_cts>10000].index.values\n",
    "df['is_commercial_by_num_trips'] = df['id'].isin(comm_by_num_trips_ids).astype(int)\n",
    "\n",
    "# - Test:\n",
    "#print(uf_id_cts.sort_values(ascending=False))\n",
    "#print(len(comm_by_num_trips_ids))\n",
    "#print(len(uf_id_cts))\n",
    "#print(uf_id_cts[uf_id_cts>10000].index.values)\n",
    "print('Test commerical vs. private classification by num of trips; following # should be zero: ' \n",
    "      + str(sum(df[df['is_commercial_by_num_trips']==0]['id'].value_counts() > 10000))) # should be zero\n",
    "\n",
    "# - Usage:\n",
    "# Get df w/o commerical by num trips --> dfnew = df[df['is_commercial_by_num_trips']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Use frequency</i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> defining 1 time users\n",
    "\n",
    "otu_ids = uf_id_cts[uf_id_cts==1].index.values\n",
    "df['is_otu'] = df['id'].isin(otu_ids).astype(int)\n",
    "\n",
    "# - Test:\n",
    "# print(uf_id_cts.sort_values(ascending=False))\n",
    "# print(len(otu_ids))\n",
    "# print(len(uf_id_cts))\n",
    "# print(len(otu_ids)/len(uf_id_cts))\n",
    "# print(uf_id_cts[uf_id_cts==1].index.values)\n",
    "uf_ids_test = df[df['is_otu']==0]['id'].value_counts()\n",
    "print('Test one-time use frequency classification; following # should be zero: ' + str(uf_ids_test[uf_ids_test==1].count())) # should be zero\n",
    "\n",
    "# - Usage:\n",
    "# Get df w/o one-time users --> dfnew = df[df['is_otu']==0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out desired user groups (currently commercial users only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnow = df[(df['is_commercial_by_num_trips']==0) & \n",
    "           (df['is_commercial_by_num_tags']==0)]\n",
    "dfnow.drop(columns=['is_commercial_by_num_trips',\n",
    "                    'is_commercial_by_num_tags',\n",
    "                    'is_otu'],\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join ACS info to cbg shapefile by fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load census block groups\n",
    "cbgs = gpd.read_file('/opt/dssg-hot/data/shapefiles/block_groups/block_groups.shp')\n",
    "cbgs.rename(columns={'fips_code':'fips'}, inplace=True)\n",
    "cbgs['fips'] = pd.to_numeric(cbgs['fips'])\n",
    "#cbgs.dtypes\n",
    "#cbgs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Load ACS 2016 info\n",
    "acs = pd.read_sql_query(\"select fips_code as fips, med_inc, population, households, race_nonhisp_white, race_nonhisp_asian, trans_carpool, trans_drivealone, trans_transit from acs;\", db)\n",
    "cols = acs.columns.drop(['fips','population'])\n",
    "acs[cols] = acs[cols].apply(pd.to_numeric, errors='coerce')\n",
    "acs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbgs_acs = cbgs.merge(acs, on='fips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in shapefiles for drawing maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in toll points shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tollptsnb = gpd.read_file('/opt/dssg-hot/data/shapefiles/toll_pts_405/northbound_405_toll_pts.shp')\n",
    "#print(tollptsnb.crs)\n",
    "#print(tollptsnb.head())\n",
    "\n",
    "majtpsnb = tollptsnb[(tollptsnb['Name']=='NB Entry 1 (entry plaza = NB1 aka 3)')\n",
    "                |(tollptsnb['Name']=='NB Exit 7 (exit plaza = NB10 aka 12)')]\n",
    "#majtpsnb.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tollptssb = gpd.read_file('/opt/dssg-hot/data/shapefiles/toll_pts_405/southbound_405_toll_pts.shp')\n",
    "#print(tollptssb.crs)\n",
    "#print(tollptssb.head())\n",
    "\n",
    "majtpssb = tollptssb[(tollptssb['Name']=='SB Entry 1 (entry plaza = SB1 aka 13)')\n",
    "                |(tollptssb['Name']=='SB Exit 7 (exit plaza = SB10 aka 23)')]\n",
    "#majtpssb.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in city points shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "citypts = gpd.read_file('/opt/dssg-hot/data/shapefiles/city_points/city_points.shp')\n",
    "#print(citypts.crs)\n",
    "citypts['geometry'] = citypts['geometry'].to_crs(epsg=4326)\n",
    "#print(citypts.head())\n",
    "\n",
    "#majcps = citypts[(citypts['MajorCity']=='yes')]\n",
    "majcps = citypts[(citypts['NAME']=='Seattle')|(citypts['NAME']=='Bellevue')\n",
    "                 |(citypts['NAME']=='Lynnwood')|(citypts['NAME']=='Woodinville')\n",
    "                 |(citypts['NAME']=='Bothell')|(citypts['NAME']=='Shoreline')\n",
    "                 |(citypts['NAME']=='Edmonds')|(citypts['NAME']=='Redmond')\n",
    "                 |(citypts['NAME']=='Kirkland')|(citypts['NAME']=='Renton')\n",
    "                 |(citypts['NAME']=='Kent')|(citypts['NAME']=='Issquah')\n",
    "                 |(citypts['NAME']=='Everett')|(citypts['NAME']=='SeaTac')]\n",
    "# ax = majcps.plot()\n",
    "# for x, y, label in zip(majcps.geometry.x, majcps.geometry.y, majcps.NAME):\n",
    "#     ax.annotate(label, xy=(x, y), xytext=(3, 3), textcoords=\"offset points\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in roads shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rds500k = gpd.read_file('/opt/dssg-hot/data/shapefiles/roads500k/sr500k_20181231.shp')\n",
    "#print(rds500k.crs)\n",
    "rds500k['geometry'] = rds500k['geometry'].to_crs(epsg=4326)\n",
    "#print(rds500k.head())\n",
    "\n",
    "majrds = rds500k[(rds500k['StateRoute']=='405')|(rds500k['StateRoute']=='005')\n",
    "                |(rds500k['StateRoute']=='522')|(rds500k['StateRoute']=='520')\n",
    "                |(rds500k['StateRoute']=='527')|(rds500k['StateRoute']=='90')]\n",
    "#majrds.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in water shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water = gpd.read_file('/opt/dssg-hot/data/shapefiles/water/water_bodies.shp')\n",
    "#print(water.crs)\n",
    "#print(water.head())\n",
    "\n",
    "majlks = water[(water['NAME']=='Green Lake')|(water['NAME']=='Lake Union')\n",
    "                |(water['NAME']=='Lake Meridian')|(water['NAME']=='Salmon Bay')\n",
    "                |(water['NAME']=='Lake Washington')|(water['NAME']=='Lake Sammamish')\n",
    "                |(water['NAME']=='Lake Youngs')|(water['NAME']=='Lake Stevens')]\n",
    "#majlks.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
